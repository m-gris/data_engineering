# Project: Data Modeling with Postgres

A collection of scripts, modules and notebooks to implement an ETL pipeline using Python and PostgreSQL.


### How to run the scripts
In a terminal simply run the following: 

First:
```bash
user_name $ python create_tables.py
```

Then 
```bash 
user_name $ python etl.py
```

### DATABASE SCHEMA 
A star schema has been implemented for queries' ease and efficiency. 

FACT TABLE: `songplay`  

DIMENSION TABLES: `users`, `songs`, `artists`, `time`


### FILES
- `create_tables.py`: script to run first to create / update the database
- `etl.py`: The actual script for the ETL pipeline
- `test.ipynb`: A notebook that allows to inspect the database
- `sql_queries.py`: module that contains sql queries for psycopg2, a python wrapper for postgresql.
- `utils.py`: module containing utility functions
- `etl.ipynb`: A notebook used to "incrementally" develop and test the etl


### FOLDERS / DATASETS: `data/song_data` & `data/log_data`
#### Song dataset
A subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```python
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
``` 

#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json



